{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ4fJMqEWUYu"
      },
      "source": [
        "# Assignment 3: Real-time Acoustic Activity Sensing (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview**\n",
        "\n",
        "In this assignment, you will build a real-time acoustic activity recognition system that continuously listens through your microphone and classifies activities on the fly. Your system will run two models (the classical ML model (A1) and the deep learning model (A2)) to compare their predictions and speeds in real time.\n",
        "\n",
        "**Learning Objectives**\n",
        "\n",
        "1. Build a real-time acoustic activity recognition system\n",
        "\n",
        "2. Compare predictions, confidence, and latency between your ML and DL models"
      ],
      "metadata": {
        "id": "qhpXSvi6HGRv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EstNWyRodDHD"
      },
      "source": [
        "\n",
        "\n",
        "In this section, you will build a real-time acoustic activity recognition system that continuously listens through your microphone and classifies activities on the fly. Your system will run two models in parallel (the classical ML model and the deep learning model) to compare their predictions and speeds in real time.\n",
        "\n",
        "## Instructions:\n",
        "\n",
        "1. Capture audio continuously from your computer’s microphone (e.g., using a library like PyAudio or sounddevice).\n",
        "2. Process the audio in sliding windows. For example, use a window length of about 1 seconds, with an overlap (hop) of ~0.5 seconds between consecutive windows. This ensures new predictions are made multiple times per second.\n",
        "3. Apply two models to each window of audio:\n",
        "  1. Your best ML classifier from A1\n",
        "  2. Your best deep learning model (from A2), or other models in the literature (example below)\n",
        "    1. [Wav2Vec 2.0](https://huggingface.co/facebook/wav2vec2-base)\n",
        "    2. [AST (Audio Spectrogram Transformer)](https://github.com/facebookresearch/AudioMAE)\n",
        "    3. [AudioMAE](https://github.com/YuanGongND/ast)\n",
        "4. Display the results in real-time, including:\n",
        "  1. A visualization of the audio waveform for the current window (updating as new audio comes in).\n",
        "  2. The predicted activity label from each model for that window, along with a confidence score or probability for each prediction.\n",
        "  3. The inference time (latency in milliseconds) it took for each model to produce the prediction for that window.\n",
        "5. Record 2–3 minutes each for step 3, including the five main activities from A1/2.\n",
        "\n",
        "Make sure to handle real-time audio carefully. Using a buffer to collect audio samples and process overlapping windows is one way to implement sliding windows. Aim to update the predictions at least ~3 times per second so the system feels responsive. You can start by printing outputs to the console, but ideally build a simple GUI to display the waveform and predictions clearly (this could be as simple as a Matplotlib plot for the waveform and text labels for predictions, or a small custom interface).\n",
        "\n",
        "The Ubicoustics GitHub repo contains some example code for your reference.\n",
        "\n",
        "## Scoring:\n",
        "\n",
        "GUI Visualization of signal **(5 points)**\n",
        "\n",
        "Real-time nature of end-to-end pipeline **(5 points)** [Aim for atleast 3 FPS]\n",
        "\n",
        "Prediction from ML classifier from A1 **(3 points)**\n",
        "\n",
        "Prediction from DL classifier from A2 **(7 points)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Real time inferencing (you would ideally want to migrate this notebook to a .py local file to use your microphone seamlessly)\n"
      ],
      "metadata": {
        "id": "XQtFsZLP5lxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion:** In your report, summarize following items:\n",
        "\n",
        "*   Describe the end-to-end pipeline: audio capture → processing → inference → display.\n",
        "*   How did you handle buffering and overlapping windows?\n",
        "*   What were the typical inference times for each model?\n",
        "*   Was one model noticeably faster or more stable?\n",
        "*   How did you visualize predictions and confidence?\n",
        "*   How well did the system respond to different environments (quiet, noisy, echo, etc.)?"
      ],
      "metadata": {
        "id": "DQxHeEQy5rMh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNb-iMjSlenD"
      },
      "source": [
        "# Submission\n",
        "\n",
        "For this assignment, please prepare the following deliverables:\n",
        "\n",
        "1.   Code Submission: Submit a zip file containing your complete code for A3.1. Include a requirements.txt file listing any Python dependencies needed to run your code (e.g., PyAudio, Transformers, Torch, etc.). Ensure that your code is well-organized and commented where appropriate.\n",
        "2.   Demonstration Videos: Provide one short video (approximately 2–3 minutes each) demonstrating your real-time system in action but performing inference for 2 ML classifiers (predictions shown in different lines on a GUI). In the video, perform each of the five target activities multiple times to showcase how the system responds. The output (GUI with visualization with two predictions below) should be clearly visible, displaying the predicted labels, confidence scores, and inference times as you perform the activities. One prediction label (including predicted activity, confidence, model latency) for best ML classifier from A1 and one for the best DL classifier for A2.\n",
        "\n",
        "The video should clearly show you performing each of the 5 activities and the system’s live output (waveform/pecogram display with predictions, confidence and latency timing). Make sure the text in your interface is readable in the video. Aim to make the demonstrations convincing that your system works correctly for each activity in real time."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g22V7Kjkx7tY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}